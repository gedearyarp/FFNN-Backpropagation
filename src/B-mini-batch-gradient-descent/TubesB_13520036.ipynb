{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar B\n",
    "IF3270 Pembelajaran Mesin<br>\n",
    "Backward Propagation - Mini Batch Gradient Descent\n",
    "\n",
    "Developed by:\n",
    "1. K01 13520010 - Ken Kalang Al Qalyubi\n",
    "2. K01 13520036 - I Gede Arya Raditya Parameswara\n",
    "3. K02 13520061 - Gibran Darmawan\n",
    "4. K03 13520119 - Marchotridyo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerEnum(Enum):\n",
    "    INPUT = \"INPUT\"\n",
    "    HIDDEN = \"HIDDEN\"\n",
    "    OUTPUT = \"OUTPUT\"\n",
    "\n",
    "class ActivationFuncEnum(Enum):\n",
    "    SIGMOID = \"SIGMOID\"\n",
    "    LINEAR = \"LINEAR\"\n",
    "    RELU = \"RELU\"\n",
    "    SOFTMAX = \"SOFTMAX\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileUtility:\n",
    "    @staticmethod\n",
    "    def import_json(file_name):\n",
    "        with open(file_name) as json_file:\n",
    "            return json.load(json_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def export_json(file_name, data):\n",
    "        with open(file_name, 'w') as outfile:\n",
    "            json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_linear(self, x):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_softmax(y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        return y - t\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # Layer adalah kelas yang menyimpan sejumlah neutron berikut fungsi aktivasinya\n",
    "    def __init__(self, neurons: list, type: str, activation_func: str):\n",
    "        self.__neurons = neurons\n",
    "        self.__type = type\n",
    "        self.__activation_func = activation_func\n",
    "\n",
    "    def add_neuron(self, neuron):\n",
    "        self.__neurons.append(neuron)\n",
    "\n",
    "    def get_neurons(self):\n",
    "        return self.__neurons\n",
    "    \n",
    "    def get_type(self):\n",
    "        return self.__type\n",
    "    \n",
    "    def get_activation_func(self):\n",
    "        return self.__activation_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(\n",
    "        self, \n",
    "        layer: Layer,\n",
    "        weight: list,\n",
    "        bias: float, \n",
    "    ):\n",
    "        self.__layer: Layer = layer\n",
    "        self.__weight: list = weight\n",
    "        self.__bias: float = bias\n",
    "        self.__net: float = 0.0\n",
    "        self.__value: float = 0.0\n",
    "\n",
    "    def activate(self):\n",
    "        if self.__layer.get_activation_func() == ActivationFuncEnum.SIGMOID.value:\n",
    "            self.__value = 1 / (1 + math.exp(-self.__net))\n",
    "        elif self.__layer.get_activation_func() == ActivationFuncEnum.LINEAR.value:\n",
    "            self.__value = self.__net\n",
    "        elif self.__layer.get_activation_func() == ActivationFuncEnum.RELU.value:\n",
    "            self.__value = max(0, self.__net)\n",
    "        elif self.__layer.get_activation_func() == ActivationFuncEnum.SOFTMAX.value:\n",
    "            layer_neurons: list = self.__layer.get_neurons()\n",
    "            exp_sum: float = 0.0\n",
    "\n",
    "            for neuron in layer_neurons:\n",
    "                exp_sum += math.exp(neuron.get_net())\n",
    "\n",
    "            self.__value = math.exp(self.__net) / exp_sum\n",
    "\n",
    "    def set_value(self, value):\n",
    "        self.__value = value\n",
    "\n",
    "    def set_net(self, net):\n",
    "        self.__net = net\n",
    "\n",
    "    def set_weight(self, index, weight):\n",
    "        self.__weight[index] = weight\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.__value\n",
    "\n",
    "    def get_net(self):\n",
    "        return self.__net\n",
    "\n",
    "    def get_weight(self, index):\n",
    "        return self.__weight[index]\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.__bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNGraph:\n",
    "    def __init__(self, layer_size: int, neuron_sizes: list, activation_func: list):\n",
    "        self.__layer_size = layer_size\n",
    "        self.__neuron_sizes = neuron_sizes\n",
    "        self.__activation_func = activation_func\n",
    "        self.__layers  = []\n",
    "        self.__outout_activation_func = activation_func[-1]\n",
    "\n",
    "        self.__build_ann_graph()\n",
    "        \n",
    "    def train(self, data: np.ndarray, target: np.ndarray ,learning_rate: float, error_threshold: float, max_epoch: int, batch_size: int = 50):\n",
    "        self.__reset_weights()\n",
    "\n",
    "        batches_x = []\n",
    "        batches_y = []\n",
    "        \n",
    "        batches_x = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
    "        batches_y = [target[i:i+batch_size] for i in range(0, len(target), batch_size)]\n",
    "        \n",
    "        batches_len = len(batches_x)\n",
    "        \n",
    "        epoch = 0\n",
    "        while epoch < max_epoch:\n",
    "            error = 0\n",
    "            random_batches_x = []\n",
    "            random_batches_y = []\n",
    "\n",
    "            random_index = random.sample(range(batches_len), batches_len)\n",
    "            for i in random_index:\n",
    "                random_batches_x.append(batches_x[i])\n",
    "                random_batches_y.append(batches_y[i])\n",
    "\n",
    "            for i in range(batches_len):\n",
    "                batch_x = random_batches_x[i]\n",
    "                batch_y = random_batches_y[i]\n",
    "\n",
    "                batch_error = 0\n",
    "\n",
    "                for j in range(len(batch_x)):\n",
    "                    self.__feed_forward(batch_x[j])\n",
    "                    batch_error += self.__error(batch_y[j])\n",
    "\n",
    "                batch_error /= len(batch_x)\n",
    "                error += batch_error\n",
    "\n",
    "                for j in range(len(batch_x)):\n",
    "                    self.__back_propagation(batch_y[j], learning_rate)\n",
    "\n",
    "            epoch += 1\n",
    "            if error < error_threshold:\n",
    "                break\n",
    "        return                \n",
    "    \n",
    "    def __error(self, target: np.array):\n",
    "        error = 0\n",
    "\n",
    "        # sesuaikan dengan nilai hasil sebenarnya pada dataset\n",
    "        target = [target[0] for i in range(len(self.__layers[-1].get_neurons()))]\n",
    "\n",
    "        if self.__outout_activation_func == ActivationFuncEnum.SOFTMAX.value:\n",
    "            epsilon = 1e-15\n",
    "            predictions = [max(epsilon, min(1 - epsilon, neuron.get_value())) for neuron in self.__layers[-1].get_neurons()]\n",
    "\n",
    "            for i in range(len(target)):\n",
    "                error += -math.log(predictions[i])\n",
    "            error /= len(target)\n",
    "\n",
    "            return error\n",
    "\n",
    "        for i in range(len(target)):\n",
    "            error += (self.__layers[-1].get_neurons()[i].get_value() - target[i]) ** 2\n",
    "        error /= 2\n",
    "        return error\n",
    "\n",
    "    def __feed_forward(self, sample: np.array):\n",
    "        for i in range(self.__layer_size):\n",
    "            if i == 0:\n",
    "                for j in range(self.__neuron_sizes[i]):\n",
    "                    self.__layers[i].get_neurons()[j].set_value(sample[j])\n",
    "            else:\n",
    "                for j in range(self.__neuron_sizes[i]):\n",
    "                    net = 0\n",
    "                    for k in range(self.__neuron_sizes[i - 1]):\n",
    "                        net += self.__layers[i - 1].get_neurons()[k].get_value() * self.__layers[i].get_neurons()[j].get_weight(k)\n",
    "                    net += self.__layers[i].get_neurons()[j].get_bias()\n",
    "\n",
    "                    self.__layers[i].get_neurons()[j].set_net(net)\n",
    "                    self.__layers[i].get_neurons()[j].activate()\n",
    "        return self.__layers         \n",
    "    \n",
    "    def __back_propagation(self, target: np.ndarray, learning_rate: float):\n",
    "        output_layer = self.__layers[-1]\n",
    "        output_layer_neurons = output_layer.get_neurons()\n",
    "        output_delta = []\n",
    "        for i in range(len(output_layer_neurons)):\n",
    "            neuron = output_layer_neurons[i]\n",
    "            if self.__outout_activation_func == ActivationFuncEnum.LINEAR.value:\n",
    "                output_delta.append(Activation.derivative_linear(neuron.get_value()) * (target[i] - neuron.get_value()))\n",
    "            if self.__outout_activation_func == ActivationFuncEnum.SIGMOID.value:\n",
    "                output_delta.append(Activation.derivative_sigmoid(neuron.get_value()) * (target[i] - neuron.get_value()))\n",
    "            if self.__outout_activation_func == ActivationFuncEnum.RELU.value:\n",
    "                output_delta.append(Activation.derivative_relu(neuron.get_value()) * (target[i] - neuron.get_value()))\n",
    "            if self.__outout_activation_func == ActivationFuncEnum.SOFTMAX.value:\n",
    "                delta_sum = 0\n",
    "                for j in range(len(output_layer_neurons)):\n",
    "                    delta_sum += (neuron.get_value() - int(i==j)) * neuron.get_weight(j) * output_delta[j]\n",
    "                output_delta.append(delta_sum)\n",
    "\n",
    "        for i in range(self.__layer_size - 2, 0, -1):\n",
    "            layer = self.__layers[i]\n",
    "            layer_neurons = layer.get_neurons()\n",
    "            next_layer = self.__layers[i + 1]\n",
    "            next_layer_neurons = next_layer.get_neurons()\n",
    "\n",
    "            delta = []\n",
    "            for j in range(len(layer_neurons)):\n",
    "                neuron = layer_neurons[j]\n",
    "                delta_sum = 0\n",
    "                for k in range(len(next_layer_neurons)):\n",
    "                    next_neuron = next_layer_neurons[k]\n",
    "                    delta_sum += next_neuron.get_weight(j) * output_delta[k]\n",
    "                if self.__hidden_activation_func == ActivationFuncEnum.LINEAR.value:\n",
    "                    delta.append(Activation.derivative_linear(neuron.get_value()) * delta_sum)\n",
    "                if self.__hidden_activation_func == ActivationFuncEnum.SIGMOID.value:\n",
    "                    delta.append(Activation.derivative_sigmoid(neuron.get_value()) * delta_sum)\n",
    "                if self.__hidden_activation_func == ActivationFuncEnum.RELU.value:\n",
    "                    delta.append(Activation.derivative_relu(neuron.get_value()) * delta_sum)\n",
    "                if self.__hidden_activation_func == ActivationFuncEnum.SOFTMAX.value:\n",
    "                    delta.append(Activation.derivative_softmax(neuron.get_value(), target[j]))\n",
    "\n",
    "            for j in range(len(layer_neurons)):\n",
    "                neuron = layer_neurons[j]\n",
    "                for k in range(len(next_layer_neurons)):\n",
    "                    next_neuron = next_layer_neurons[k]\n",
    "                    delta_weight = learning_rate * output_delta[k] * neuron.get_value()\n",
    "                    new_weight = next_neuron.get_weight(j) + delta_weight\n",
    "                    next_neuron.set_weight(j, new_weight)\n",
    "\n",
    "            output_delta = delta\n",
    "\n",
    "\n",
    "        input_layer = self.__layers[0]\n",
    "        input_layer_neurons = input_layer.get_neurons()\n",
    "        next_layer = self.__layers[1]\n",
    "        next_layer_neurons = next_layer.get_neurons()\n",
    "\n",
    "        for i in range(len(input_layer_neurons)):\n",
    "            neuron = input_layer_neurons[i]\n",
    "            for j in range(len(next_layer_neurons)):\n",
    "                next_neuron = next_layer_neurons[j]\n",
    "                delta_weight = learning_rate * output_delta[j] * neuron.get_value()\n",
    "                new_weight = next_neuron.get_weight(i) + delta_weight\n",
    "                next_neuron.set_weight(i, new_weight)\n",
    "    \n",
    "    def __build_ann_graph(self):\n",
    "        for i in range(self.__layer_size):\n",
    "            if i == 0:\n",
    "                layer = Layer([], LayerEnum.INPUT.value, self.__activation_func[i])\n",
    "            elif i == (self.__layer_size - 1):\n",
    "                layer = Layer([], LayerEnum.OUTPUT.value, self.__activation_func[i])\n",
    "            else:\n",
    "                layer = Layer([], LayerEnum.HIDDEN.value, self.__activation_func[i])\n",
    "\n",
    "            for j in range(self.__neuron_sizes[i]):\n",
    "                weight = []\n",
    "                \n",
    "                if i > 0:\n",
    "                    for k in range(self.__neuron_sizes[i - 1] + 1):\n",
    "                        weight.append(np.random.uniform(-0.5, 0.5))\n",
    "                \n",
    "                neuron = Neuron(layer, weight, 1)\n",
    "                layer.add_neuron(neuron)\n",
    "            \n",
    "            self.__layers.append(layer)\n",
    "\n",
    "        return self.__layers\n",
    "    \n",
    "    def __reset_weights(self):\n",
    "        for i in range(self.__layer_size):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            for j in range(self.__neuron_sizes[i]):\n",
    "                for k in range(self.__neuron_sizes[i - 1] + 1):\n",
    "                    self.__layers[i].get_neurons()[j].set_weight(k, np.random.uniform(-0.5, 0.5))\n",
    "    \n",
    "    def draw_ann_graph(self):\n",
    "        # Terminologies:\n",
    "        # Xi = neuron ke-i di input layer\n",
    "        # Hij = neuron ke-j di hidden layer ke-i\n",
    "        # Oi = neuron ke-i di output layer\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Proses setiap layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            prev_layer = self.layers[i - 1]\n",
    "            prev_prefix = \"\"\n",
    "            prefix = \"\"\n",
    "\n",
    "            if prev_layer.get_type() == LayerEnum.INPUT.value:\n",
    "                prev_prefix = \"X\"\n",
    "            elif prev_layer.get_type() == LayerEnum.HIDDEN.value:\n",
    "                prev_prefix = f\"H{i - 1}\"\n",
    "            else:\n",
    "                prev_prefix = \"O\"\n",
    "            \n",
    "            if layer.get_type() == LayerEnum.INPUT.value:\n",
    "                prefix = \"X\"\n",
    "            elif layer.get_type() == LayerEnum.HIDDEN.value:\n",
    "                prefix = f\"H{i}\"\n",
    "            else:\n",
    "                prefix = \"O\"\n",
    "\n",
    "            # Tambahkan edge dari setiap neuron di prev_layer ke layer\n",
    "            for j, _ in enumerate(prev_layer.get_neurons()):\n",
    "                for k, neuron in enumerate(layer.get_neurons()):\n",
    "                    if j == 0:\n",
    "                        print(f\"Bobot bias untuk {prefix}{k + 1} = {neuron.get_weight(0)}\")\n",
    "                    G.add_edge(f\"{prev_prefix}{j + 1}\", f\"{prefix}{k + 1}\", weight=neuron.get_weight(j + 1))\n",
    "            \n",
    "        # Set posisi node graph\n",
    "        pos = {}\n",
    "        curr_x = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            curr_y = 0\n",
    "\n",
    "            prefix = \"\"\n",
    "            if layer.get_type() == LayerEnum.INPUT.value:\n",
    "                prefix = \"X\"\n",
    "            elif layer.get_type() == LayerEnum.HIDDEN.value:\n",
    "                prefix = f\"H{i}\"\n",
    "            else:\n",
    "                prefix = \"O\"\n",
    "            \n",
    "            for j, _ in enumerate(layer.get_neurons()):\n",
    "                pos[f\"{prefix}{j + 1}\"] = (curr_x, curr_y)\n",
    "                curr_y += 1\n",
    "\n",
    "            curr_x += 1\n",
    "\n",
    "        options = {\n",
    "            \"font_size\": 12,\n",
    "            \"node_size\": 2000,\n",
    "            \"node_color\": \"white\",\n",
    "            \"edgecolors\": \"black\",\n",
    "            \"linewidths\": 5,\n",
    "            \"width\": 5,\n",
    "        }\n",
    "\n",
    "        nx.draw_networkx(G, pos, **options)\n",
    "        edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, label_pos=0.6)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.margins(0.2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "graph = ANNGraph(3, [len(iris.feature_names), 2, len(iris.target_names)], [None, ActivationFuncEnum.SIGMOID.value, ActivationFuncEnum.SOFTMAX.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[307], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph\u001b[39m.\u001b[39;49mtrain(x, y\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m), \u001b[39m1e-2\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, \u001b[39m1000\u001b[39;49m, \u001b[39m50\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[305], line 47\u001b[0m, in \u001b[0;36mANNGraph.train\u001b[0;34m(self, data, target, learning_rate, error_threshold, max_epoch, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m     error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_error\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(batch_x)):\n\u001b[0;32m---> 47\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__back_propagation(batch_y[j], learning_rate)\n\u001b[1;32m     49\u001b[0m epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39m<\u001b[39m error_threshold:\n",
      "Cell \u001b[0;32mIn[305], line 106\u001b[0m, in \u001b[0;36mANNGraph.__back_propagation\u001b[0;34m(self, target, learning_rate)\u001b[0m\n\u001b[1;32m    104\u001b[0m         delta_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(output_layer_neurons)):\n\u001b[0;32m--> 106\u001b[0m             delta_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (neuron\u001b[39m.\u001b[39mget_value() \u001b[39m-\u001b[39m \u001b[39mint\u001b[39m(i\u001b[39m==\u001b[39mj)) \u001b[39m*\u001b[39m neuron\u001b[39m.\u001b[39mget_weight(j) \u001b[39m*\u001b[39m output_delta[j]\n\u001b[1;32m    107\u001b[0m         output_delta\u001b[39m.\u001b[39mappend(delta_sum)\n\u001b[1;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__layer_size \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "graph.train(x, y.reshape(-1,1), 1e-2, 0.1, 1000, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
