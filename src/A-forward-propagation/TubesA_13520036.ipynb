{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar A\n",
    "IF3270 Pembelajaran Mesin<br>\n",
    "Forward Propagation - Feed Forward Neural Network (FFNN)\n",
    "\n",
    "Developed by:\n",
    "1. K01 13520010 - Ken Kalang Al Qalyubi\n",
    "2. K01 13520036 - I Gede Arya Raditya Parameswara\n",
    "3. K02 13520061 - Gibran Darmawan\n",
    "4. K03 13520119 - Marchotridyo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "from enum import Enum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerEnum(Enum):\n",
    "    INPUT = \"INPUT\"\n",
    "    HIDDEN = \"HIDDEN\"\n",
    "    OUTPUT = \"OUTPUT\"\n",
    "\n",
    "class ActivationFuncEnum(Enum):\n",
    "    SIGMOID = \"SIGMOID\"\n",
    "    LINEAR = \"LINEAR\"\n",
    "    RELU = \"RELU\"\n",
    "    SOFTMAX = \"SOFTMAX\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileUtility:\n",
    "    @staticmethod\n",
    "    def import_json(file_name):\n",
    "        with open(file_name) as json_file:\n",
    "            return json.load(json_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def export_json(file_name, data):\n",
    "        with open(file_name, 'w') as outfile:\n",
    "            json.dump(data, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(\n",
    "        self, \n",
    "        layer_type: str, \n",
    "        activation_func: str, \n",
    "        weight: list,\n",
    "        bias: list, \n",
    "    ):\n",
    "        self.__layer_type: LayerEnum = layer_type\n",
    "        self.__activation_func: ActivationFuncEnum = activation_func\n",
    "        self.__weight: list = weight\n",
    "        self.__bias: list = bias\n",
    "        self.__net: float = 0.0\n",
    "        self.__value: float = 0.0\n",
    "\n",
    "    def activate(self):\n",
    "        if self.__activation_func == ActivationFuncEnum.SIGMOID.value:\n",
    "            self.__value = 1 / (1 + math.exp(-self.__net))\n",
    "        elif self.__activation_func == ActivationFuncEnum.LINEAR.value:\n",
    "            self.__value = self.__net\n",
    "        elif self.__activation_func == ActivationFuncEnum.RELU.value:\n",
    "            self.__value = max(0, self.__net)\n",
    "        elif self.__activation_func == ActivationFuncEnum.SOFTMAX.value:\n",
    "            # TODO: Ini harusnya mengakses semua .__net dari neuron pada layer yang sama\n",
    "            self.__value = math.exp(self.__net) / sum([math.exp(net) for net in self.__net])\n",
    "\n",
    "    def set_value(self, value):\n",
    "        self.__value = value\n",
    "\n",
    "    def set_net(self, net):\n",
    "        self.__net = net\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.__value\n",
    "\n",
    "    def get_net(self):\n",
    "        return self.__net\n",
    "\n",
    "    def get_weight(self, index):\n",
    "        return self.__weight[index]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.__weight\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def get_layer_type(self):\n",
    "        return self.__layer_type\n",
    "\n",
    "    def get_activation_func(self):\n",
    "        return self.__activation_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNGraph:\n",
    "    def __init__(self, file_config_path: str):\n",
    "        self.file_path = file_config_path\n",
    "        self.config = None\n",
    "\n",
    "        self.layers = []\n",
    "        self.build_ann_graph()\n",
    "\n",
    "    def build_ann_graph(self):\n",
    "        self.config = FileUtility.import_json(self.file_path)\n",
    "        layers = self.config[\"layers\"]\n",
    "\n",
    "        for layer in layers:\n",
    "            current_layer = []\n",
    "\n",
    "            for i in range(layer[\"total_neurons\"]):\n",
    "                neuron = self.__generate_neuron_data(layer, i)\n",
    "                current_layer.append(neuron)\n",
    "\n",
    "            self.layers.append(current_layer)\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        for i in range(len(input_data)):\n",
    "            # Masukkan input_data ke neuron di input layer\n",
    "            neuron: Neuron = self.layers[0][i]\n",
    "            neuron.set_value(input_data[i])\n",
    "\n",
    "        self.__activate_all_neurons()\n",
    "\n",
    "        return\n",
    "\n",
    "    def print_details(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(\"--------------------\")\n",
    "            print(\"Layer\", i+1)\n",
    "            print(\"--------------------\")\n",
    "            for j in range(len(self.layers[i])):\n",
    "                neuron: Neuron = self.layers[i][j]\n",
    "                if j == 0:\n",
    "                    print(\"Activation Function:\", neuron.get_activation_func())\n",
    "                print(f\"[Neuron {j+1}] Net:\", neuron.get_net())\n",
    "                print(f\"[Neuron {j+1}] Value:\", neuron.get_value())\n",
    "            print(\"\")\n",
    "\n",
    "    def __generate_neuron_data(self, layer, i):\n",
    "        layer_type: str = layer[\"type\"]\n",
    "        activation_func: str = None\n",
    "        weight: list = []\n",
    "        bias: list = []\n",
    "\n",
    "        if (layer_type == LayerEnum.HIDDEN.value or layer_type == LayerEnum.OUTPUT.value):\n",
    "            activation_func = layer[\"activation_func\"]\n",
    "            weight = layer[\"weight\"][i]\n",
    "            bias = layer[\"bias\"][i]\n",
    "\n",
    "        return Neuron(layer_type, activation_func, weight, bias)\n",
    "    \n",
    "    def __activate_all_neurons(self):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            for j in range(len(self.layers[i])):\n",
    "                neuron: Neuron = self.layers[i][j]\n",
    "                previous_layer = self.layers[i - 1]\n",
    "\n",
    "                self.__generate_neuron_net(neuron, previous_layer)\n",
    "                neuron.activate()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __generate_neuron_net(self, neuron: Neuron, previous_layer: list):\n",
    "        total = 0.0\n",
    "\n",
    "        for i in range(len(previous_layer)):\n",
    "            total += previous_layer[i].get_value() * neuron.get_weight(i)\n",
    "\n",
    "        # TODO: Tambahkan weight untuk bias\n",
    "        total += neuron.get_bias()\n",
    "        neuron.set_net(total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Layer 1\n",
      "--------------------\n",
      "Activation Function: None\n",
      "[Neuron 1] Net: 0.0\n",
      "[Neuron 1] Value: 0\n",
      "[Neuron 2] Net: 0.0\n",
      "[Neuron 2] Value: 0\n",
      "\n",
      "--------------------\n",
      "Layer 2\n",
      "--------------------\n",
      "Activation Function: SIGMOID\n",
      "[Neuron 1] Net: -10.0\n",
      "[Neuron 1] Value: 4.5397868702434395e-05\n",
      "[Neuron 2] Net: 30.0\n",
      "[Neuron 2] Value: 0.9999999999999065\n",
      "\n",
      "--------------------\n",
      "Layer 3\n",
      "--------------------\n",
      "Activation Function: SIGMOID\n",
      "[Neuron 1] Net: -9.999092042627819\n",
      "[Neuron 1] Value: 4.543910487654591e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = ANNGraph(\"config/xor_sigmoid.json\")\n",
    "graph.predict([0, 0])\n",
    "graph.print_details()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
